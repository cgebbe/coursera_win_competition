{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.15.2\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgebbe/anaconda3/envs/coursera_winkaggle2/lib/python3.5/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgebbe/anaconda3/envs/coursera_winkaggle2/lib/python3.5/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1081fffb8f4299997e790fcbef1b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5) # date_block_num in [12,33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3354,)\n",
      "(3354, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "print(pred_lr.shape)\n",
    "print(X_test_level2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34404,)\n",
      "(34404,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120192    27\n",
       "120193    27\n",
       "120194    27\n",
       "120195    27\n",
       "120196    27\n",
       "Name: date_block_num, dtype: int32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = dates_train.isin([27, 28, 29, 30, 31, 32])\n",
    "dates_train_level2 = dates_train[mask]\n",
    "y_train_level2 = y_train[mask]\n",
    "print(dates_train_level2.shape)\n",
    "print(y_train_level2.shape)\n",
    "dates_train_level2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    print(cur_block_num)\n",
    "    \n",
    "    # 1. split X_train into parts\n",
    "    last_block = cur_block_num\n",
    "    dates_train = dates[dates <  last_block]\n",
    "    dates_test  = dates[dates == last_block]\n",
    "    X_train2 = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "    X_test2 =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "    y_train2 = all_data.loc[dates <  last_block, 'target'].values\n",
    "    y_test2 =  all_data.loc[dates == last_block, 'target'].values\n",
    "    \n",
    "    # 2,3. Fit LR and LightGBM\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train2.values, y_train2)\n",
    "    pred_lr = lr.predict(X_test2.values)\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train2, label=y_train2), 100)\n",
    "    pred_lgb = model.predict(X_test2)\n",
    "    X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "    \n",
    "    # 4. Store at right place\n",
    "    mask = dates_train_level2 == cur_block_num\n",
    "    X_train_level2[mask, : ] = X_test_level2\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooray, correct\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "if np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989])):\n",
    "    print(\"Hooray, correct\")\n",
    "else:\n",
    "    print(\"boo, bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0dea2c8198>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(1,2, figsize=(10,5))\n",
    "_ = axes[0].hist(X_train_level2, bins=np.arange(0,100))\n",
    "_ = axes[1].hist(y_train_level2, bins=np.arange(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found better a=0.0 yielding r2=0.4710187874149271\n",
      "Found better a=0.001 yielding r2=0.47142713181186047\n",
      "Found better a=0.002 yielding r2=0.47183494187895925\n",
      "Found better a=0.003 yielding r2=0.4722422176162233\n",
      "Found better a=0.004 yielding r2=0.47264895902365245\n",
      "Found better a=0.005 yielding r2=0.47305516610124676\n",
      "Found better a=0.006 yielding r2=0.47346083884900647\n",
      "Found better a=0.007 yielding r2=0.47386597726693136\n",
      "Found better a=0.008 yielding r2=0.4742705813550213\n",
      "Found better a=0.009000000000000001 yielding r2=0.47467465111327634\n",
      "Found better a=0.01 yielding r2=0.47507818654169687\n",
      "Found better a=0.011 yielding r2=0.47548118764028247\n",
      "Found better a=0.012 yielding r2=0.47588365440903324\n",
      "Found better a=0.013000000000000001 yielding r2=0.4762855868479493\n",
      "Found better a=0.014 yielding r2=0.47668698495703044\n",
      "Found better a=0.015 yielding r2=0.47708784873627696\n",
      "Found better a=0.016 yielding r2=0.47748817818568856\n",
      "Found better a=0.017 yielding r2=0.47788797330526533\n",
      "Found better a=0.018000000000000002 yielding r2=0.4782872340950073\n",
      "Found better a=0.019 yielding r2=0.47868596055491464\n",
      "Found better a=0.02 yielding r2=0.47908415268498694\n",
      "Found better a=0.021 yielding r2=0.47948181048522454\n",
      "Found better a=0.022 yielding r2=0.4798789339556274\n",
      "Found better a=0.023 yielding r2=0.48027552309619537\n",
      "Found better a=0.024 yielding r2=0.4806715779069285\n",
      "Found better a=0.025 yielding r2=0.481067098387827\n",
      "Found better a=0.026000000000000002 yielding r2=0.4814620845388907\n",
      "Found better a=0.027 yielding r2=0.4818565363601195\n",
      "Found better a=0.028 yielding r2=0.48225045385151366\n",
      "Found better a=0.029 yielding r2=0.4826438370130728\n",
      "Found better a=0.03 yielding r2=0.4830366858447972\n",
      "Found better a=0.031 yielding r2=0.4834290003466868\n",
      "Found better a=0.032 yielding r2=0.48382078051874167\n",
      "Found better a=0.033 yielding r2=0.4842120263609617\n",
      "Found better a=0.034 yielding r2=0.48460273787334707\n",
      "Found better a=0.035 yielding r2=0.4849929150558975\n",
      "Found better a=0.036000000000000004 yielding r2=0.4853825579086132\n",
      "Found better a=0.037 yielding r2=0.48577166643149394\n",
      "Found better a=0.038 yielding r2=0.4861602406245401\n",
      "Found better a=0.039 yielding r2=0.48654828048775134\n",
      "Found better a=0.04 yielding r2=0.48693578602112775\n",
      "Found better a=0.041 yielding r2=0.48732275722466956\n",
      "Found better a=0.042 yielding r2=0.4877091940983763\n",
      "Found better a=0.043000000000000003 yielding r2=0.48809509664224837\n",
      "Found better a=0.044 yielding r2=0.4884804648562856\n",
      "Found better a=0.045 yielding r2=0.4888652987404881\n",
      "Found better a=0.046 yielding r2=0.4892495982948557\n",
      "Found better a=0.047 yielding r2=0.4896333635193887\n",
      "Found better a=0.048 yielding r2=0.49001659441408674\n",
      "Found better a=0.049 yielding r2=0.4903992909789501\n",
      "Found better a=0.05 yielding r2=0.4907814532139785\n",
      "Found better a=0.051000000000000004 yielding r2=0.4911630811191722\n",
      "Found better a=0.052000000000000005 yielding r2=0.49154417469453116\n",
      "Found better a=0.053 yielding r2=0.4919247339400551\n",
      "Found better a=0.054 yielding r2=0.49230475885574454\n",
      "Found better a=0.055 yielding r2=0.49268424944159905\n",
      "Found better a=0.056 yielding r2=0.49306320569761863\n",
      "Found better a=0.057 yielding r2=0.4934416276238035\n",
      "Found better a=0.058 yielding r2=0.49381951522015366\n",
      "Found better a=0.059000000000000004 yielding r2=0.494196868486669\n",
      "Found better a=0.06 yielding r2=0.4945736874233496\n",
      "Found better a=0.061 yielding r2=0.4949499720301953\n",
      "Found better a=0.062 yielding r2=0.49532572230720606\n",
      "Found better a=0.063 yielding r2=0.49570093825438233\n",
      "Found better a=0.064 yielding r2=0.49607561987172366\n",
      "Found better a=0.065 yielding r2=0.49644976715923006\n",
      "Found better a=0.066 yielding r2=0.49682338011690186\n",
      "Found better a=0.067 yielding r2=0.49719645874473883\n",
      "Found better a=0.068 yielding r2=0.4975690030427409\n",
      "Found better a=0.069 yielding r2=0.4979410130109082\n",
      "Found better a=0.07 yielding r2=0.4983124886492407\n",
      "Found better a=0.07100000000000001 yielding r2=0.4986834299577384\n",
      "Found better a=0.07200000000000001 yielding r2=0.4990538369364014\n",
      "Found better a=0.073 yielding r2=0.4994237095852295\n",
      "Found better a=0.074 yielding r2=0.49979304790422274\n",
      "Found better a=0.075 yielding r2=0.5001618518933814\n",
      "Found better a=0.076 yielding r2=0.5005301215527052\n",
      "Found better a=0.077 yielding r2=0.5008978568821941\n",
      "Found better a=0.078 yielding r2=0.5012650578818483\n",
      "Found better a=0.079 yielding r2=0.5016317245516675\n",
      "Found better a=0.08 yielding r2=0.5019978568916521\n",
      "Found better a=0.081 yielding r2=0.5023634549018019\n",
      "Found better a=0.082 yielding r2=0.5027285185821169\n",
      "Found better a=0.083 yielding r2=0.503093047932597\n",
      "Found better a=0.084 yielding r2=0.5034570429532423\n",
      "Found better a=0.085 yielding r2=0.5038205036440528\n",
      "Found better a=0.08600000000000001 yielding r2=0.5041834300050285\n",
      "Found better a=0.08700000000000001 yielding r2=0.5045458220361696\n",
      "Found better a=0.088 yielding r2=0.5049076797374759\n",
      "Found better a=0.089 yielding r2=0.5052690031089472\n",
      "Found better a=0.09 yielding r2=0.5056297921505838\n",
      "Found better a=0.091 yielding r2=0.5059900468623855\n",
      "Found better a=0.092 yielding r2=0.5063497672443525\n",
      "Found better a=0.093 yielding r2=0.5067089532964848\n",
      "Found better a=0.094 yielding r2=0.507067605018782\n",
      "Found better a=0.095 yielding r2=0.5074257224112446\n",
      "Found better a=0.096 yielding r2=0.5077833054738725\n",
      "Found better a=0.097 yielding r2=0.5081403542066654\n",
      "Found better a=0.098 yielding r2=0.5084968686096236\n",
      "Found better a=0.099 yielding r2=0.5088528486827468\n",
      "Found better a=0.1 yielding r2=0.5092082944260355\n",
      "Found better a=0.101 yielding r2=0.5095632058394896\n",
      "Found better a=0.10200000000000001 yielding r2=0.5099175829231084\n",
      "Found better a=0.10300000000000001 yielding r2=0.5102714256768927\n",
      "Found better a=0.10400000000000001 yielding r2=0.510624734100842\n",
      "Found better a=0.105 yielding r2=0.5109775081949566\n",
      "Found better a=0.106 yielding r2=0.5113297479592364\n",
      "Found better a=0.107 yielding r2=0.5116814533936813\n",
      "Found better a=0.108 yielding r2=0.5120326244982917\n",
      "Found better a=0.109 yielding r2=0.512383261273067\n",
      "Found better a=0.11 yielding r2=0.5127333637180076\n",
      "Found better a=0.111 yielding r2=0.5130829318331135\n",
      "Found better a=0.112 yielding r2=0.5134319656183846\n",
      "Found better a=0.113 yielding r2=0.5137804650738207\n",
      "Found better a=0.114 yielding r2=0.5141284301994222\n",
      "Found better a=0.115 yielding r2=0.5144758609951887\n",
      "Found better a=0.116 yielding r2=0.5148227574611206\n",
      "Found better a=0.117 yielding r2=0.5151691195972177\n",
      "Found better a=0.11800000000000001 yielding r2=0.5155149474034799\n",
      "Found better a=0.11900000000000001 yielding r2=0.5158602408799073\n",
      "Found better a=0.12 yielding r2=0.5162050000264999\n",
      "Found better a=0.121 yielding r2=0.5165492248432578\n",
      "Found better a=0.122 yielding r2=0.5168929153301809\n",
      "Found better a=0.123 yielding r2=0.5172360714872691\n",
      "Found better a=0.124 yielding r2=0.5175786933145226\n",
      "Found better a=0.125 yielding r2=0.517920780811941\n",
      "Found better a=0.126 yielding r2=0.5182623339795251\n",
      "Found better a=0.127 yielding r2=0.518603352817274\n",
      "Found better a=0.128 yielding r2=0.5189438373251883\n",
      "Found better a=0.129 yielding r2=0.5192837875032678\n",
      "Found better a=0.13 yielding r2=0.5196232033515125\n",
      "Found better a=0.131 yielding r2=0.5199620848699223\n",
      "Found better a=0.132 yielding r2=0.5203004320584974\n",
      "Found better a=0.133 yielding r2=0.5206382449172378\n",
      "Found better a=0.134 yielding r2=0.5209755234461432\n",
      "Found better a=0.135 yielding r2=0.5213122676452138\n",
      "Found better a=0.136 yielding r2=0.5216484775144496\n",
      "Found better a=0.137 yielding r2=0.5219841530538507\n",
      "Found better a=0.138 yielding r2=0.5223192942634169\n",
      "Found better a=0.139 yielding r2=0.5226539011431486\n",
      "Found better a=0.14 yielding r2=0.5229879736930452\n",
      "Found better a=0.14100000000000001 yielding r2=0.5233215119131072\n",
      "Found better a=0.14200000000000002 yielding r2=0.5236545158033341\n",
      "Found better a=0.14300000000000002 yielding r2=0.5239869853637265\n",
      "Found better a=0.14400000000000002 yielding r2=0.524318920594284\n",
      "Found better a=0.145 yielding r2=0.5246503214950067\n",
      "Found better a=0.146 yielding r2=0.5249811880658946\n",
      "Found better a=0.147 yielding r2=0.5253115203069476\n",
      "Found better a=0.148 yielding r2=0.5256413182181658\n",
      "Found better a=0.149 yielding r2=0.5259705817995495\n",
      "Found better a=0.15 yielding r2=0.5262993110510982\n",
      "Found better a=0.151 yielding r2=0.526627505972812\n",
      "Found better a=0.152 yielding r2=0.5269551665646911\n",
      "Found better a=0.153 yielding r2=0.5272822928267356\n",
      "Found better a=0.154 yielding r2=0.527608884758945\n",
      "Found better a=0.155 yielding r2=0.5279349423613198\n",
      "Found better a=0.156 yielding r2=0.5282604656338596\n",
      "Found better a=0.157 yielding r2=0.5285854545765647\n",
      "Found better a=0.158 yielding r2=0.528909909189435\n",
      "Found better a=0.159 yielding r2=0.5292338294724706\n",
      "Found better a=0.16 yielding r2=0.5295572154256712\n",
      "Found better a=0.161 yielding r2=0.5298800670490371\n",
      "Found better a=0.162 yielding r2=0.5302023843425683\n",
      "Found better a=0.163 yielding r2=0.5305241673062647\n",
      "Found better a=0.164 yielding r2=0.5308454159401262\n",
      "Found better a=0.165 yielding r2=0.5311661302441529\n",
      "Found better a=0.166 yielding r2=0.5314863102183449\n",
      "Found better a=0.167 yielding r2=0.5318059558627021\n",
      "Found better a=0.168 yielding r2=0.5321250671772244\n",
      "Found better a=0.169 yielding r2=0.5324436441619119\n",
      "Found better a=0.17 yielding r2=0.5327616868167646\n",
      "Found better a=0.171 yielding r2=0.5330791951417826\n",
      "Found better a=0.17200000000000001 yielding r2=0.5333961691369657\n",
      "Found better a=0.17300000000000001 yielding r2=0.5337126088023141\n",
      "Found better a=0.17400000000000002 yielding r2=0.5340285141378277\n",
      "Found better a=0.17500000000000002 yielding r2=0.5343438851435065\n",
      "Found better a=0.176 yielding r2=0.5346587218193504\n",
      "Found better a=0.177 yielding r2=0.5349730241653596\n",
      "Found better a=0.178 yielding r2=0.535286792181534\n",
      "Found better a=0.179 yielding r2=0.5356000258678735\n",
      "Found better a=0.18 yielding r2=0.5359127252243783\n",
      "Found better a=0.181 yielding r2=0.5362248902510482\n",
      "Found better a=0.182 yielding r2=0.5365365209478834\n",
      "Found better a=0.183 yielding r2=0.536847617314884\n",
      "Found better a=0.184 yielding r2=0.5371581793520495\n",
      "Found better a=0.185 yielding r2=0.5374682070593801\n",
      "Found better a=0.186 yielding r2=0.5377777004368761\n",
      "Found better a=0.187 yielding r2=0.5380866594845374\n",
      "Found better a=0.188 yielding r2=0.5383950842023637\n",
      "Found better a=0.189 yielding r2=0.5387029745903553\n",
      "Found better a=0.19 yielding r2=0.5390103306485121\n",
      "Found better a=0.191 yielding r2=0.5393171523768341\n",
      "Found better a=0.192 yielding r2=0.5396234397753212\n",
      "Found better a=0.193 yielding r2=0.5399291928439738\n",
      "Found better a=0.194 yielding r2=0.5402344115827913\n",
      "Found better a=0.195 yielding r2=0.5405390959917742\n",
      "Found better a=0.196 yielding r2=0.540843246070922\n",
      "Found better a=0.197 yielding r2=0.5411468618202353\n",
      "Found better a=0.198 yielding r2=0.5414499432397137\n",
      "Found better a=0.199 yielding r2=0.5417524903293574\n",
      "Found better a=0.2 yielding r2=0.5420545030891661\n",
      "Found better a=0.201 yielding r2=0.5423559815191402\n",
      "Found better a=0.202 yielding r2=0.5426569256192794\n",
      "Found better a=0.203 yielding r2=0.5429573353895838\n",
      "Found better a=0.20400000000000001 yielding r2=0.5432572108300535\n",
      "Found better a=0.20500000000000002 yielding r2=0.5435565519406882\n",
      "Found better a=0.20600000000000002 yielding r2=0.5438553587214883\n",
      "Found better a=0.20700000000000002 yielding r2=0.5441536311724535\n",
      "Found better a=0.20800000000000002 yielding r2=0.544451369293584\n",
      "Found better a=0.209 yielding r2=0.5447485730848796\n",
      "Found better a=0.21 yielding r2=0.5450452425463403\n",
      "Found better a=0.211 yielding r2=0.5453413776779664\n",
      "Found better a=0.212 yielding r2=0.5456369784797577\n",
      "Found better a=0.213 yielding r2=0.5459320449517142\n",
      "Found better a=0.214 yielding r2=0.5462265770938357\n",
      "Found better a=0.215 yielding r2=0.5465205749061226\n",
      "Found better a=0.216 yielding r2=0.5468140383885747\n",
      "Found better a=0.217 yielding r2=0.5471069675411919\n",
      "Found better a=0.218 yielding r2=0.5473993623639744\n",
      "Found better a=0.219 yielding r2=0.5476912228569221\n",
      "Found better a=0.22 yielding r2=0.547982549020035\n",
      "Found better a=0.221 yielding r2=0.5482733408533129\n",
      "Found better a=0.222 yielding r2=0.5485635983567563\n",
      "Found better a=0.223 yielding r2=0.5488533215303648\n",
      "Found better a=0.224 yielding r2=0.5491425103741384\n",
      "Found better a=0.225 yielding r2=0.5494311648880772\n",
      "Found better a=0.226 yielding r2=0.5497192850721813\n",
      "Found better a=0.227 yielding r2=0.5500068709264506\n",
      "Found better a=0.228 yielding r2=0.550293922450885\n",
      "Found better a=0.229 yielding r2=0.5505804396454848\n",
      "Found better a=0.23 yielding r2=0.5508664225102495\n",
      "Found better a=0.231 yielding r2=0.5511518710451797\n",
      "Found better a=0.232 yielding r2=0.5514367852502751\n",
      "Found better a=0.233 yielding r2=0.5517211651255355\n",
      "Found better a=0.234 yielding r2=0.5520050106709611\n",
      "Found better a=0.23500000000000001 yielding r2=0.5522883218865521\n",
      "Found better a=0.23600000000000002 yielding r2=0.5525710987723081\n",
      "Found better a=0.23700000000000002 yielding r2=0.5528533413282295\n",
      "Found better a=0.23800000000000002 yielding r2=0.553135049554316\n",
      "Found better a=0.23900000000000002 yielding r2=0.5534162234505677\n",
      "Found better a=0.24 yielding r2=0.5536968630169846\n",
      "Found better a=0.241 yielding r2=0.5539769682535667\n",
      "Found better a=0.242 yielding r2=0.554256539160314\n",
      "Found better a=0.243 yielding r2=0.5545355757372266\n",
      "Found better a=0.244 yielding r2=0.5548140779843043\n",
      "Found better a=0.245 yielding r2=0.5550920459015471\n",
      "Found better a=0.246 yielding r2=0.5553694794889552\n",
      "Found better a=0.247 yielding r2=0.5556463787465286\n",
      "Found better a=0.248 yielding r2=0.5559227436742671\n",
      "Found better a=0.249 yielding r2=0.5561985742721708\n",
      "Found better a=0.25 yielding r2=0.5564738705402398\n",
      "Found better a=0.251 yielding r2=0.5567486324784738\n",
      "Found better a=0.252 yielding r2=0.5570228600868732\n",
      "Found better a=0.253 yielding r2=0.5572965533654377\n",
      "Found better a=0.254 yielding r2=0.5575697123141674\n",
      "Found better a=0.255 yielding r2=0.5578423369330624\n",
      "Found better a=0.256 yielding r2=0.5581144272221226\n",
      "Found better a=0.257 yielding r2=0.5583859831813478\n",
      "Found better a=0.258 yielding r2=0.5586570048107383\n",
      "Found better a=0.259 yielding r2=0.5589274921102941\n",
      "Found better a=0.26 yielding r2=0.5591974450800151\n",
      "Found better a=0.261 yielding r2=0.5594668637199012\n",
      "Found better a=0.262 yielding r2=0.5597357480299526\n",
      "Found better a=0.263 yielding r2=0.5600040980101691\n",
      "Found better a=0.264 yielding r2=0.5602719136605507\n",
      "Found better a=0.265 yielding r2=0.5605391949810978\n",
      "Found better a=0.266 yielding r2=0.56080594197181\n",
      "Found better a=0.267 yielding r2=0.5610721546326873\n",
      "Found better a=0.268 yielding r2=0.5613378329637299\n",
      "Found better a=0.269 yielding r2=0.5616029769649378\n",
      "Found better a=0.27 yielding r2=0.5618675866363106\n",
      "Found better a=0.271 yielding r2=0.5621316619778489\n",
      "Found better a=0.272 yielding r2=0.5623952029895523\n",
      "Found better a=0.273 yielding r2=0.5626582096714208\n",
      "Found better a=0.274 yielding r2=0.5629206820234546\n",
      "Found better a=0.275 yielding r2=0.5631826200456536\n",
      "Found better a=0.276 yielding r2=0.5634440237380178\n",
      "Found better a=0.277 yielding r2=0.563704893100547\n",
      "Found better a=0.278 yielding r2=0.5639652281332417\n",
      "Found better a=0.279 yielding r2=0.5642250288361015\n",
      "Found better a=0.28 yielding r2=0.5644842952091265\n",
      "Found better a=0.281 yielding r2=0.5647430272523166\n",
      "Found better a=0.28200000000000003 yielding r2=0.5650012249656721\n",
      "Found better a=0.28300000000000003 yielding r2=0.5652588883491927\n",
      "Found better a=0.28400000000000003 yielding r2=0.5655160174028786\n",
      "Found better a=0.28500000000000003 yielding r2=0.5657726121267295\n",
      "Found better a=0.28600000000000003 yielding r2=0.5660286725207455\n",
      "Found better a=0.28700000000000003 yielding r2=0.5662841985849272\n",
      "Found better a=0.28800000000000003 yielding r2=0.5665391903192738\n",
      "Found better a=0.289 yielding r2=0.5667936477237855\n",
      "Found better a=0.29 yielding r2=0.5670475707984626\n",
      "Found better a=0.291 yielding r2=0.5673009595433047\n",
      "Found better a=0.292 yielding r2=0.5675538139583121\n",
      "Found better a=0.293 yielding r2=0.5678061340434848\n",
      "Found better a=0.294 yielding r2=0.5680579197988225\n",
      "Found better a=0.295 yielding r2=0.5683091712243256\n",
      "Found better a=0.296 yielding r2=0.5685598883199938\n",
      "Found better a=0.297 yielding r2=0.5688100710858273\n",
      "Found better a=0.298 yielding r2=0.5690597195218259\n",
      "Found better a=0.299 yielding r2=0.5693088336279897\n",
      "Found better a=0.3 yielding r2=0.5695574134043186\n",
      "Found better a=0.301 yielding r2=0.569805458850813\n",
      "Found better a=0.302 yielding r2=0.5700529699674723\n",
      "Found better a=0.303 yielding r2=0.5702999467542971\n",
      "Found better a=0.304 yielding r2=0.570546389211287\n",
      "Found better a=0.305 yielding r2=0.5707922973384418\n",
      "Found better a=0.306 yielding r2=0.5710376711357622\n",
      "Found better a=0.307 yielding r2=0.5712825106032475\n",
      "Found better a=0.308 yielding r2=0.5715268157408984\n",
      "Found better a=0.309 yielding r2=0.5717705865487142\n",
      "Found better a=0.31 yielding r2=0.5720138230266952\n",
      "Found better a=0.311 yielding r2=0.5722565251748415\n",
      "Found better a=0.312 yielding r2=0.5724986929931529\n",
      "Found better a=0.313 yielding r2=0.5727403264816295\n",
      "Found better a=0.314 yielding r2=0.5729814256402714\n",
      "Found better a=0.315 yielding r2=0.5732219904690785\n",
      "Found better a=0.316 yielding r2=0.5734620209680508\n",
      "Found better a=0.317 yielding r2=0.5737015171371882\n",
      "Found better a=0.318 yielding r2=0.573940478976491\n",
      "Found better a=0.319 yielding r2=0.5741789064859588\n",
      "Found better a=0.32 yielding r2=0.5744167996655919\n",
      "Found better a=0.321 yielding r2=0.5746541585153901\n",
      "Found better a=0.322 yielding r2=0.5748909830353537\n",
      "Found better a=0.323 yielding r2=0.5751272732254824\n",
      "Found better a=0.324 yielding r2=0.5753630290857763\n",
      "Found better a=0.325 yielding r2=0.5755982506162354\n",
      "Found better a=0.326 yielding r2=0.5758329378168596\n",
      "Found better a=0.327 yielding r2=0.576067090687649\n",
      "Found better a=0.328 yielding r2=0.5763007092286039\n",
      "Found better a=0.329 yielding r2=0.5765337934397238\n",
      "Found better a=0.33 yielding r2=0.5767663433210088\n",
      "Found better a=0.331 yielding r2=0.5769983588724592\n",
      "Found better a=0.332 yielding r2=0.5772298400940745\n",
      "Found better a=0.333 yielding r2=0.5774607869858553\n",
      "Found better a=0.334 yielding r2=0.5776911995478013\n",
      "Found better a=0.335 yielding r2=0.5779210777799124\n",
      "Found better a=0.336 yielding r2=0.5781504216821887\n",
      "Found better a=0.337 yielding r2=0.5783792312546302\n",
      "Found better a=0.338 yielding r2=0.5786075064972369\n",
      "Found better a=0.339 yielding r2=0.5788352474100089\n",
      "Found better a=0.34 yielding r2=0.579062453992946\n",
      "Found better a=0.341 yielding r2=0.5792891262460482\n",
      "Found better a=0.342 yielding r2=0.5795152641693158\n",
      "Found better a=0.343 yielding r2=0.5797408677627486\n",
      "Found better a=0.34400000000000003 yielding r2=0.5799659370263464\n",
      "Found better a=0.34500000000000003 yielding r2=0.5801904719601096\n",
      "Found better a=0.34600000000000003 yielding r2=0.5804144725640379\n",
      "Found better a=0.34700000000000003 yielding r2=0.5806379388381315\n",
      "Found better a=0.34800000000000003 yielding r2=0.5808608707823902\n",
      "Found better a=0.34900000000000003 yielding r2=0.5810832683968141\n",
      "Found better a=0.35000000000000003 yielding r2=0.5813051316814033\n",
      "Found better a=0.35100000000000003 yielding r2=0.5815264606361576\n",
      "Found better a=0.352 yielding r2=0.5817472552610772\n",
      "Found better a=0.353 yielding r2=0.5819675155561619\n",
      "Found better a=0.354 yielding r2=0.582187241521412\n",
      "Found better a=0.355 yielding r2=0.582406433156827\n",
      "Found better a=0.356 yielding r2=0.5826250904624074\n",
      "Found better a=0.357 yielding r2=0.582843213438153\n",
      "Found better a=0.358 yielding r2=0.5830608020840637\n",
      "Found better a=0.359 yielding r2=0.5832778564001397\n",
      "Found better a=0.36 yielding r2=0.5834943763863809\n",
      "Found better a=0.361 yielding r2=0.5837103620427873\n",
      "Found better a=0.362 yielding r2=0.5839258133693588\n",
      "Found better a=0.363 yielding r2=0.5841407303660955\n",
      "Found better a=0.364 yielding r2=0.5843551130329975\n",
      "Found better a=0.365 yielding r2=0.5845689613700648\n",
      "Found better a=0.366 yielding r2=0.5847822753772971\n",
      "Found better a=0.367 yielding r2=0.5849950550546947\n",
      "Found better a=0.368 yielding r2=0.5852073004022575\n",
      "Found better a=0.369 yielding r2=0.5854190114199853\n",
      "Found better a=0.37 yielding r2=0.5856301881078786\n",
      "Found better a=0.371 yielding r2=0.585840830465937\n",
      "Found better a=0.372 yielding r2=0.5860509384941606\n",
      "Found better a=0.373 yielding r2=0.5862605121925495\n",
      "Found better a=0.374 yielding r2=0.5864695515611034\n",
      "Found better a=0.375 yielding r2=0.5866780565998226\n",
      "Found better a=0.376 yielding r2=0.5868860273087071\n",
      "Found better a=0.377 yielding r2=0.5870934636877565\n",
      "Found better a=0.378 yielding r2=0.5873003657369714\n",
      "Found better a=0.379 yielding r2=0.5875067334563515\n",
      "Found better a=0.38 yielding r2=0.5877125668458967\n",
      "Found better a=0.381 yielding r2=0.5879178659056071\n",
      "Found better a=0.382 yielding r2=0.5881226306354828\n",
      "Found better a=0.383 yielding r2=0.5883268610355235\n",
      "Found better a=0.384 yielding r2=0.5885305571057295\n",
      "Found better a=0.385 yielding r2=0.5887337188461007\n",
      "Found better a=0.386 yielding r2=0.588936346256637\n",
      "Found better a=0.387 yielding r2=0.5891384393373388\n",
      "Found better a=0.388 yielding r2=0.5893399980882056\n",
      "Found better a=0.389 yielding r2=0.5895410225092377\n",
      "Found better a=0.39 yielding r2=0.5897415126004348\n",
      "Found better a=0.391 yielding r2=0.5899414683617974\n",
      "Found better a=0.392 yielding r2=0.5901408897933249\n",
      "Found better a=0.393 yielding r2=0.5903397768950178\n",
      "Found better a=0.394 yielding r2=0.5905381296668758\n",
      "Found better a=0.395 yielding r2=0.5907359481088991\n",
      "Found better a=0.396 yielding r2=0.5909332322210874\n",
      "Found better a=0.397 yielding r2=0.5911299820034411\n",
      "Found better a=0.398 yielding r2=0.59132619745596\n",
      "Found better a=0.399 yielding r2=0.591521878578644\n",
      "Found better a=0.4 yielding r2=0.5917170253714934\n",
      "Found better a=0.401 yielding r2=0.5919116378345077\n",
      "Found better a=0.402 yielding r2=0.5921057159676875\n",
      "Found better a=0.403 yielding r2=0.5922992597710323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found better a=0.404 yielding r2=0.5924922692445425\n",
      "Found better a=0.405 yielding r2=0.5926847443882177\n",
      "Found better a=0.406 yielding r2=0.5928766852020582\n",
      "Found better a=0.40700000000000003 yielding r2=0.5930680916860638\n",
      "Found better a=0.40800000000000003 yielding r2=0.5932589638402346\n",
      "Found better a=0.40900000000000003 yielding r2=0.5934493016645705\n",
      "Found better a=0.41000000000000003 yielding r2=0.593639105159072\n",
      "Found better a=0.41100000000000003 yielding r2=0.5938283743237385\n",
      "Found better a=0.41200000000000003 yielding r2=0.5940171091585702\n",
      "Found better a=0.41300000000000003 yielding r2=0.594205309663567\n",
      "Found better a=0.41400000000000003 yielding r2=0.5943929758387291\n",
      "Found better a=0.41500000000000004 yielding r2=0.5945801076840564\n",
      "Found better a=0.41600000000000004 yielding r2=0.5947667051995489\n",
      "Found better a=0.417 yielding r2=0.5949527683852067\n",
      "Found better a=0.418 yielding r2=0.5951382972410295\n",
      "Found better a=0.419 yielding r2=0.5953232917670176\n",
      "Found better a=0.42 yielding r2=0.5955077519631709\n",
      "Found better a=0.421 yielding r2=0.5956916778294894\n",
      "Found better a=0.422 yielding r2=0.595875069365973\n",
      "Found better a=0.423 yielding r2=0.596057926572622\n",
      "Found better a=0.424 yielding r2=0.5962402494494361\n",
      "Found better a=0.425 yielding r2=0.5964220379964154\n",
      "Found better a=0.426 yielding r2=0.5966032922135599\n",
      "Found better a=0.427 yielding r2=0.5967840121008696\n",
      "Found better a=0.428 yielding r2=0.5969641976583445\n",
      "Found better a=0.429 yielding r2=0.5971438488859846\n",
      "Found better a=0.43 yielding r2=0.59732296578379\n",
      "Found better a=0.431 yielding r2=0.5975015483517605\n",
      "Found better a=0.432 yielding r2=0.5976795965898962\n",
      "Found better a=0.433 yielding r2=0.5978571104981971\n",
      "Found better a=0.434 yielding r2=0.5980340900766632\n",
      "Found better a=0.435 yielding r2=0.5982105353252947\n",
      "Found better a=0.436 yielding r2=0.5983864462440911\n",
      "Found better a=0.437 yielding r2=0.5985618228330528\n",
      "Found better a=0.438 yielding r2=0.5987366650921799\n",
      "Found better a=0.439 yielding r2=0.5989109730214719\n",
      "Found better a=0.44 yielding r2=0.5990847466209294\n",
      "Found better a=0.441 yielding r2=0.5992579858905518\n",
      "Found better a=0.442 yielding r2=0.5994306908303396\n",
      "Found better a=0.443 yielding r2=0.5996028614402924\n",
      "Found better a=0.444 yielding r2=0.5997744977204107\n",
      "Found better a=0.445 yielding r2=0.5999455996706939\n",
      "Found better a=0.446 yielding r2=0.6001161672911426\n",
      "Found better a=0.447 yielding r2=0.6002862005817563\n",
      "Found better a=0.448 yielding r2=0.6004556995425353\n",
      "Found better a=0.449 yielding r2=0.6006246641734796\n",
      "Found better a=0.45 yielding r2=0.6007930944745888\n",
      "Found better a=0.451 yielding r2=0.6009609904458634\n",
      "Found better a=0.452 yielding r2=0.6011283520873032\n",
      "Found better a=0.453 yielding r2=0.6012951793989081\n",
      "Found better a=0.454 yielding r2=0.6014614723806783\n",
      "Found better a=0.455 yielding r2=0.6016272310326137\n",
      "Found better a=0.456 yielding r2=0.6017924553547143\n",
      "Found better a=0.457 yielding r2=0.6019571453469801\n",
      "Found better a=0.458 yielding r2=0.6021213010094111\n",
      "Found better a=0.459 yielding r2=0.6022849223420073\n",
      "Found better a=0.46 yielding r2=0.6024480093447686\n",
      "Found better a=0.461 yielding r2=0.6026105620176951\n",
      "Found better a=0.462 yielding r2=0.602772580360787\n",
      "Found better a=0.463 yielding r2=0.602934064374044\n",
      "Found better a=0.464 yielding r2=0.6030950140574661\n",
      "Found better a=0.465 yielding r2=0.6032554294110536\n",
      "Found better a=0.466 yielding r2=0.6034153104348061\n",
      "Found better a=0.467 yielding r2=0.603574657128724\n",
      "Found better a=0.468 yielding r2=0.6037334694928069\n",
      "Found better a=0.46900000000000003 yielding r2=0.6038917475270552\n",
      "Found better a=0.47000000000000003 yielding r2=0.6040494912314687\n",
      "Found better a=0.47100000000000003 yielding r2=0.6042067006060472\n",
      "Found better a=0.47200000000000003 yielding r2=0.604363375650791\n",
      "Found better a=0.47300000000000003 yielding r2=0.6045195163657\n",
      "Found better a=0.47400000000000003 yielding r2=0.6046751227507743\n",
      "Found better a=0.47500000000000003 yielding r2=0.6048301948060137\n",
      "Found better a=0.47600000000000003 yielding r2=0.6049847325314183\n",
      "Found better a=0.47700000000000004 yielding r2=0.6051387359269881\n",
      "Found better a=0.47800000000000004 yielding r2=0.6052922049927232\n",
      "Found better a=0.47900000000000004 yielding r2=0.6054451397286233\n",
      "Found better a=0.48 yielding r2=0.6055975401346888\n",
      "Found better a=0.481 yielding r2=0.6057494062109194\n",
      "Found better a=0.482 yielding r2=0.6059007379573152\n",
      "Found better a=0.483 yielding r2=0.6060515353738762\n",
      "Found better a=0.484 yielding r2=0.6062017984606025\n",
      "Found better a=0.485 yielding r2=0.606351527217494\n",
      "Found better a=0.486 yielding r2=0.6065007216445506\n",
      "Found better a=0.487 yielding r2=0.6066493817417724\n",
      "Found better a=0.488 yielding r2=0.6067975075091594\n",
      "Found better a=0.489 yielding r2=0.6069450989467118\n",
      "Found better a=0.49 yielding r2=0.6070921560544292\n",
      "Found better a=0.491 yielding r2=0.6072386788323119\n",
      "Found better a=0.492 yielding r2=0.6073846672803598\n",
      "Found better a=0.493 yielding r2=0.6075301213985729\n",
      "Found better a=0.494 yielding r2=0.607675041186951\n",
      "Found better a=0.495 yielding r2=0.6078194266454945\n",
      "Found better a=0.496 yielding r2=0.6079632777742032\n",
      "Found better a=0.497 yielding r2=0.608106594573077\n",
      "Found better a=0.498 yielding r2=0.6082493770421162\n",
      "Found better a=0.499 yielding r2=0.6083916251813204\n",
      "Found better a=0.5 yielding r2=0.60853333899069\n",
      "Found better a=0.501 yielding r2=0.6086745184702245\n",
      "Found better a=0.502 yielding r2=0.6088151636199244\n",
      "Found better a=0.503 yielding r2=0.6089552744397895\n",
      "Found better a=0.504 yielding r2=0.6090948509298197\n",
      "Found better a=0.505 yielding r2=0.6092338930900152\n",
      "Found better a=0.506 yielding r2=0.609372400920376\n",
      "Found better a=0.507 yielding r2=0.6095103744209018\n",
      "Found better a=0.508 yielding r2=0.609647813591593\n",
      "Found better a=0.509 yielding r2=0.6097847184324492\n",
      "Found better a=0.51 yielding r2=0.6099210889434707\n",
      "Found better a=0.511 yielding r2=0.6100569251246575\n",
      "Found better a=0.512 yielding r2=0.6101922269760094\n",
      "Found better a=0.513 yielding r2=0.6103269944975265\n",
      "Found better a=0.514 yielding r2=0.6104612276892087\n",
      "Found better a=0.515 yielding r2=0.6105949265510562\n",
      "Found better a=0.516 yielding r2=0.6107280910830689\n",
      "Found better a=0.517 yielding r2=0.6108607212852468\n",
      "Found better a=0.518 yielding r2=0.61099281715759\n",
      "Found better a=0.519 yielding r2=0.6111243787000984\n",
      "Found better a=0.52 yielding r2=0.6112554059127718\n",
      "Found better a=0.521 yielding r2=0.6113858987956104\n",
      "Found better a=0.522 yielding r2=0.6115158573486144\n",
      "Found better a=0.523 yielding r2=0.6116452815717836\n",
      "Found better a=0.524 yielding r2=0.6117741714651179\n",
      "Found better a=0.525 yielding r2=0.6119025270286174\n",
      "Found better a=0.526 yielding r2=0.6120303482622822\n",
      "Found better a=0.527 yielding r2=0.6121576351661121\n",
      "Found better a=0.528 yielding r2=0.6122843877401072\n",
      "Found better a=0.529 yielding r2=0.6124106059842676\n",
      "Found better a=0.53 yielding r2=0.6125362898985931\n",
      "Found better a=0.531 yielding r2=0.6126614394830838\n",
      "Found better a=0.532 yielding r2=0.6127860547377397\n",
      "Found better a=0.533 yielding r2=0.612910135662561\n",
      "Found better a=0.534 yielding r2=0.6130336822575473\n",
      "Found better a=0.535 yielding r2=0.6131566945226989\n",
      "Found better a=0.536 yielding r2=0.6132791724580156\n",
      "Found better a=0.537 yielding r2=0.6134011160634975\n",
      "Found better a=0.538 yielding r2=0.6135225253391448\n",
      "Found better a=0.539 yielding r2=0.6136434002849571\n",
      "Found better a=0.54 yielding r2=0.6137637409009347\n",
      "Found better a=0.541 yielding r2=0.6138835471870774\n",
      "Found better a=0.542 yielding r2=0.6140028191433853\n",
      "Found better a=0.543 yielding r2=0.6141215567698586\n",
      "Found better a=0.544 yielding r2=0.6142397600664969\n",
      "Found better a=0.545 yielding r2=0.6143574290333005\n",
      "Found better a=0.546 yielding r2=0.6144745636702693\n",
      "Found better a=0.547 yielding r2=0.6145911639774031\n",
      "Found better a=0.548 yielding r2=0.6147072299547025\n",
      "Found better a=0.549 yielding r2=0.6148227616021669\n",
      "Found better a=0.55 yielding r2=0.6149377589197964\n",
      "Found better a=0.551 yielding r2=0.6150522219075911\n",
      "Found better a=0.552 yielding r2=0.6151661505655511\n",
      "Found better a=0.553 yielding r2=0.6152795448936763\n",
      "Found better a=0.554 yielding r2=0.6153924048919668\n",
      "Found better a=0.555 yielding r2=0.6155047305604223\n",
      "Found better a=0.556 yielding r2=0.615616521899043\n",
      "Found better a=0.557 yielding r2=0.615727778907829\n",
      "Found better a=0.558 yielding r2=0.6158385015867802\n",
      "Found better a=0.559 yielding r2=0.6159486899358968\n",
      "Found better a=0.56 yielding r2=0.6160583439551784\n",
      "Found better a=0.561 yielding r2=0.616167463644625\n",
      "Found better a=0.562 yielding r2=0.6162760490042372\n",
      "Found better a=0.5630000000000001 yielding r2=0.6163841000340143\n",
      "Found better a=0.5640000000000001 yielding r2=0.6164916167339567\n",
      "Found better a=0.5650000000000001 yielding r2=0.6165985991040644\n",
      "Found better a=0.5660000000000001 yielding r2=0.6167050471443373\n",
      "Found better a=0.5670000000000001 yielding r2=0.6168109608547753\n",
      "Found better a=0.5680000000000001 yielding r2=0.6169163402353786\n",
      "Found better a=0.5690000000000001 yielding r2=0.6170211852861469\n",
      "Found better a=0.5700000000000001 yielding r2=0.6171254960070806\n",
      "Found better a=0.5710000000000001 yielding r2=0.6172292723981794\n",
      "Found better a=0.5720000000000001 yielding r2=0.6173325144594435\n",
      "Found better a=0.5730000000000001 yielding r2=0.6174352221908725\n",
      "Found better a=0.5740000000000001 yielding r2=0.617537395592467\n",
      "Found better a=0.5750000000000001 yielding r2=0.6176390346642266\n",
      "Found better a=0.5760000000000001 yielding r2=0.6177401394061516\n",
      "Found better a=0.577 yielding r2=0.6178407098182416\n",
      "Found better a=0.578 yielding r2=0.6179407459004969\n",
      "Found better a=0.579 yielding r2=0.6180402476529172\n",
      "Found better a=0.58 yielding r2=0.618139215075503\n",
      "Found better a=0.581 yielding r2=0.6182376481682537\n",
      "Found better a=0.582 yielding r2=0.6183355469311698\n",
      "Found better a=0.583 yielding r2=0.618432911364251\n",
      "Found better a=0.584 yielding r2=0.6185297414674975\n",
      "Found better a=0.585 yielding r2=0.6186260372409093\n",
      "Found better a=0.586 yielding r2=0.618721798684486\n",
      "Found better a=0.587 yielding r2=0.6188170257982282\n",
      "Found better a=0.588 yielding r2=0.6189117185821355\n",
      "Found better a=0.589 yielding r2=0.6190058770362079\n",
      "Found better a=0.59 yielding r2=0.6190995011604455\n",
      "Found better a=0.591 yielding r2=0.6191925909548484\n",
      "Found better a=0.592 yielding r2=0.6192851464194165\n",
      "Found better a=0.593 yielding r2=0.6193771675541497\n",
      "Found better a=0.594 yielding r2=0.6194686543590482\n",
      "Found better a=0.595 yielding r2=0.6195596068341119\n",
      "Found better a=0.596 yielding r2=0.6196500249793409\n",
      "Found better a=0.597 yielding r2=0.619739908794735\n",
      "Found better a=0.598 yielding r2=0.6198292582802942\n",
      "Found better a=0.599 yielding r2=0.6199180734360187\n",
      "Found better a=0.6 yielding r2=0.6200063542619083\n",
      "Found better a=0.601 yielding r2=0.6200941007579632\n",
      "Found better a=0.602 yielding r2=0.6201813129241833\n",
      "Found better a=0.603 yielding r2=0.6202679907605688\n",
      "Found better a=0.604 yielding r2=0.6203541342671193\n",
      "Found better a=0.605 yielding r2=0.6204397434438349\n",
      "Found better a=0.606 yielding r2=0.6205248182907158\n",
      "Found better a=0.607 yielding r2=0.6206093588077619\n",
      "Found better a=0.608 yielding r2=0.6206933649949733\n",
      "Found better a=0.609 yielding r2=0.6207768368523499\n",
      "Found better a=0.61 yielding r2=0.6208597743798915\n",
      "Found better a=0.611 yielding r2=0.6209421775775983\n",
      "Found better a=0.612 yielding r2=0.6210240464454704\n",
      "Found better a=0.613 yielding r2=0.6211053809835078\n",
      "Found better a=0.614 yielding r2=0.6211861811917104\n",
      "Found better a=0.615 yielding r2=0.621266447070078\n",
      "Found better a=0.616 yielding r2=0.621346178618611\n",
      "Found better a=0.617 yielding r2=0.6214253758373092\n",
      "Found better a=0.618 yielding r2=0.6215040387261725\n",
      "Found better a=0.619 yielding r2=0.6215821672852009\n",
      "Found better a=0.62 yielding r2=0.6216597615143947\n",
      "Found better a=0.621 yielding r2=0.6217368214137536\n",
      "Found better a=0.622 yielding r2=0.6218133469832778\n",
      "Found better a=0.623 yielding r2=0.6218893382229671\n",
      "Found better a=0.624 yielding r2=0.6219647951328217\n",
      "Found better a=0.625 yielding r2=0.6220397177128416\n",
      "Found better a=0.626 yielding r2=0.6221141059630264\n",
      "Found better a=0.627 yielding r2=0.6221879598833766\n",
      "Found better a=0.628 yielding r2=0.6222612794738919\n",
      "Found better a=0.629 yielding r2=0.6223340647345725\n",
      "Found better a=0.63 yielding r2=0.6224063156654183\n",
      "Found better a=0.631 yielding r2=0.6224780322664292\n",
      "Found better a=0.632 yielding r2=0.6225492145376054\n",
      "Found better a=0.633 yielding r2=0.6226198624789467\n",
      "Found better a=0.634 yielding r2=0.6226899760904533\n",
      "Found better a=0.635 yielding r2=0.622759555372125\n",
      "Found better a=0.636 yielding r2=0.622828600323962\n",
      "Found better a=0.637 yielding r2=0.6228971109459642\n",
      "Found better a=0.638 yielding r2=0.6229650872381316\n",
      "Found better a=0.639 yielding r2=0.6230325292004641\n",
      "Found better a=0.64 yielding r2=0.623099436832962\n",
      "Found better a=0.641 yielding r2=0.623165810135625\n",
      "Found better a=0.642 yielding r2=0.6232316491084531\n",
      "Found better a=0.643 yielding r2=0.6232969537514466\n",
      "Found better a=0.644 yielding r2=0.6233617240646051\n",
      "Found better a=0.645 yielding r2=0.6234259600479288\n",
      "Found better a=0.646 yielding r2=0.623489661701418\n",
      "Found better a=0.647 yielding r2=0.6235528290250721\n",
      "Found better a=0.648 yielding r2=0.6236154620188915\n",
      "Found better a=0.649 yielding r2=0.6236775606828762\n",
      "Found better a=0.65 yielding r2=0.6237391250170259\n",
      "Found better a=0.651 yielding r2=0.6238001550213409\n",
      "Found better a=0.652 yielding r2=0.6238606506958212\n",
      "Found better a=0.653 yielding r2=0.6239206120404666\n",
      "Found better a=0.654 yielding r2=0.6239800390552772\n",
      "Found better a=0.655 yielding r2=0.6240389317402529\n",
      "Found better a=0.656 yielding r2=0.624097290095394\n",
      "Found better a=0.657 yielding r2=0.6241551141207002\n",
      "Found better a=0.658 yielding r2=0.6242124038161716\n",
      "Found better a=0.659 yielding r2=0.6242691591818083\n",
      "Found better a=0.66 yielding r2=0.6243253802176101\n",
      "Found better a=0.661 yielding r2=0.6243810669235771\n",
      "Found better a=0.662 yielding r2=0.6244362192997093\n",
      "Found better a=0.663 yielding r2=0.6244908373460067\n",
      "Found better a=0.664 yielding r2=0.6245449210624694\n",
      "Found better a=0.665 yielding r2=0.6245984704490972\n",
      "Found better a=0.666 yielding r2=0.6246514855058903\n",
      "Found better a=0.667 yielding r2=0.6247039662328484\n",
      "Found better a=0.668 yielding r2=0.624755912629972\n",
      "Found better a=0.669 yielding r2=0.6248073246972605\n",
      "Found better a=0.67 yielding r2=0.6248582024347145\n",
      "Found better a=0.671 yielding r2=0.6249085458423336\n",
      "Found better a=0.672 yielding r2=0.6249583549201179\n",
      "Found better a=0.673 yielding r2=0.6250076296680673\n",
      "Found better a=0.674 yielding r2=0.6250563700861819\n",
      "Found better a=0.675 yielding r2=0.6251045761744617\n",
      "Found better a=0.676 yielding r2=0.6251522479329068\n",
      "Found better a=0.677 yielding r2=0.625199385361517\n",
      "Found better a=0.678 yielding r2=0.6252459884602926\n",
      "Found better a=0.679 yielding r2=0.6252920572292331\n",
      "Found better a=0.68 yielding r2=0.625337591668339\n",
      "Found better a=0.681 yielding r2=0.6253825917776101\n",
      "Found better a=0.682 yielding r2=0.6254270575570464\n",
      "Found better a=0.683 yielding r2=0.6254709890066478\n",
      "Found better a=0.684 yielding r2=0.6255143861264145\n",
      "Found better a=0.685 yielding r2=0.6255572489163463\n",
      "Found better a=0.686 yielding r2=0.6255995773764434\n",
      "Found better a=0.687 yielding r2=0.6256413715067057\n",
      "Found better a=0.6880000000000001 yielding r2=0.6256826313071333\n",
      "Found better a=0.6890000000000001 yielding r2=0.6257233567777261\n",
      "Found better a=0.6900000000000001 yielding r2=0.6257635479184839\n",
      "Found better a=0.6910000000000001 yielding r2=0.625803204729407\n",
      "Found better a=0.6920000000000001 yielding r2=0.6258423272104952\n",
      "Found better a=0.6930000000000001 yielding r2=0.6258809153617488\n",
      "Found better a=0.6940000000000001 yielding r2=0.6259189691831675\n",
      "Found better a=0.6950000000000001 yielding r2=0.6259564886747514\n",
      "Found better a=0.6960000000000001 yielding r2=0.6259934738365005\n",
      "Found better a=0.6970000000000001 yielding r2=0.6260299246684147\n",
      "Found better a=0.6980000000000001 yielding r2=0.6260658411704942\n",
      "Found better a=0.6990000000000001 yielding r2=0.626101223342739\n",
      "Found better a=0.7000000000000001 yielding r2=0.6261360711851489\n",
      "Found better a=0.7010000000000001 yielding r2=0.626170384697724\n",
      "Found better a=0.7020000000000001 yielding r2=0.6262041638804643\n",
      "Found better a=0.7030000000000001 yielding r2=0.6262374087333699\n",
      "Found better a=0.704 yielding r2=0.6262701192564406\n",
      "Found better a=0.705 yielding r2=0.6263022954496764\n",
      "Found better a=0.706 yielding r2=0.6263339373130776\n",
      "Found better a=0.707 yielding r2=0.626365044846644\n",
      "Found better a=0.708 yielding r2=0.6263956180503756\n",
      "Found better a=0.709 yielding r2=0.6264256569242723\n",
      "Found better a=0.71 yielding r2=0.6264551614683342\n",
      "Found better a=0.711 yielding r2=0.6264841316825613\n",
      "Found better a=0.712 yielding r2=0.6265125675669536\n",
      "Found better a=0.713 yielding r2=0.6265404691215113\n",
      "Found better a=0.714 yielding r2=0.6265678363462339\n",
      "Found better a=0.715 yielding r2=0.6265946692411218\n",
      "Found better a=0.716 yielding r2=0.6266209678061749\n",
      "Found better a=0.717 yielding r2=0.6266467320413933\n",
      "Found better a=0.718 yielding r2=0.6266719619467769\n",
      "Found better a=0.719 yielding r2=0.6266966575223256\n",
      "Found better a=0.72 yielding r2=0.6267208187680396\n",
      "Found better a=0.721 yielding r2=0.6267444456839188\n",
      "Found better a=0.722 yielding r2=0.6267675382699631\n",
      "Found better a=0.723 yielding r2=0.6267900965261728\n",
      "Found better a=0.724 yielding r2=0.6268121204525474\n",
      "Found better a=0.725 yielding r2=0.6268336100490876\n",
      "Found better a=0.726 yielding r2=0.6268545653157926\n",
      "Found better a=0.727 yielding r2=0.626874986252663\n",
      "Found better a=0.728 yielding r2=0.6268948728596986\n",
      "Found better a=0.729 yielding r2=0.6269142251368994\n",
      "Found better a=0.73 yielding r2=0.6269330430842653\n",
      "Found better a=0.731 yielding r2=0.6269513267017965\n",
      "Found better a=0.732 yielding r2=0.626969075989493\n",
      "Found better a=0.733 yielding r2=0.6269862909473545\n",
      "Found better a=0.734 yielding r2=0.6270029715753813\n",
      "Found better a=0.735 yielding r2=0.6270191178735733\n",
      "Found better a=0.736 yielding r2=0.6270347298419304\n",
      "Found better a=0.737 yielding r2=0.6270498074804529\n",
      "Found better a=0.738 yielding r2=0.6270643507891405\n",
      "Found better a=0.739 yielding r2=0.6270783597679933\n",
      "Found better a=0.74 yielding r2=0.6270918344170112\n",
      "Found better a=0.741 yielding r2=0.6271047747361946\n",
      "Found better a=0.742 yielding r2=0.6271171807255429\n",
      "Found better a=0.743 yielding r2=0.6271290523850565\n",
      "Found better a=0.744 yielding r2=0.6271403897147354\n",
      "Found better a=0.745 yielding r2=0.6271511927145793\n",
      "Found better a=0.746 yielding r2=0.6271614613845885\n",
      "Found better a=0.747 yielding r2=0.627171195724763\n",
      "Found better a=0.748 yielding r2=0.6271803957351025\n",
      "Found better a=0.749 yielding r2=0.6271890614156075\n",
      "Found better a=0.75 yielding r2=0.6271971927662774\n",
      "Found better a=0.751 yielding r2=0.6272047897871127\n",
      "Found better a=0.752 yielding r2=0.627211852478113\n",
      "Found better a=0.753 yielding r2=0.6272183808392786\n",
      "Found better a=0.754 yielding r2=0.6272243748706094\n",
      "Found better a=0.755 yielding r2=0.6272298345721056\n",
      "Found better a=0.756 yielding r2=0.6272347599437669\n",
      "Found better a=0.757 yielding r2=0.6272391509855932\n",
      "Found better a=0.758 yielding r2=0.627243007697585\n",
      "Found better a=0.759 yielding r2=0.6272463300797417\n",
      "Found better a=0.76 yielding r2=0.6272491181320637\n",
      "Found better a=0.761 yielding r2=0.627251371854551\n",
      "Found better a=0.762 yielding r2=0.6272530912472034\n",
      "Found better a=0.763 yielding r2=0.6272542763100212\n",
      "Found better a=0.764 yielding r2=0.627254927043004\n",
      "Found better a=0.765 yielding r2=0.6272550434461521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "best_alpha = None\n",
    "best_r2 = 0\n",
    "for a in alphas_to_try:\n",
    "    y = X_train_level2[:,0] * a + X_train_level2[:,1] * (1-a)\n",
    "    r2 = r2_score(y_train_level2, y)\n",
    "    if r2>best_r2:\n",
    "        best_r2=r2\n",
    "        best_alpha=a\n",
    "        print(\"Found better a={} yielding r2={}\".format(a, r2))\n",
    "    \n",
    "r2_train_simple_mix = best_r2\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3354,)\n",
      "(3354,)\n",
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "test_preds = X_test_level2[:,0] * best_alpha + X_test_level2[:,1] * (1-best_alpha)\n",
    "print(test_preds.shape)\n",
    "print(y_test.shape)\n",
    "r2_test_simple_mix = r2_score(y_test,test_preds)\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_lvl2 = LinearRegression()\n",
    "lr_lvl2.fit(X_train_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "train_preds = lr_lvl2.predict(X_train_level2)\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds)\n",
    "\n",
    "test_preds = lr_lvl2.predict(X_test_level2)\n",
    "r2_test_stacking = r2_score(y_test, test_preds)\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.6272550434461521\n",
      "Current answer for task r2_test_simple_mix is: 0.7811441695790362\n",
      "Current answer for task r2_train_stacking is: 0.6321755614590683\n",
      "Current answer for task r2_test_stacking is: 0.771297132342246\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task best_alpha: 0.765\n",
      "Task r2_train_simple_mix: 0.6272550434461521\n",
      "Task r2_test_simple_mix: 0.7811441695790362\n",
      "Task r2_train_stacking: 0.6321755614590683\n",
      "Task r2_test_stacking: 0.771297132342246\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'christian.gebbe@gmail.com'\n",
    "STUDENT_TOKEN = 'zgfMY8gzGS5jxsFB'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
